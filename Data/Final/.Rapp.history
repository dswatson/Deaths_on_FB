hist(null_distro, breaks = 50)
cpi_hat
null_1 <- null_distro
x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }#
  # Estimate CPI#
  full_loss <- loss(dat)#
  dat0 <- dat #
  dat0[, 1] <- dat[sample.int(n), 1]#
  null_loss <- loss(dat0)#
  cpi_hat <- mean(null_loss - full_loss)#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
       W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  null_2 <- c(null_distro, cpi_hat)
hist(null_2)
hist(null_2, breaks = 50)
cpi_hat
hist(null_1, breaks = 50)
hist(null_2, breaks = 50)
x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }#
  # Estimate CPI#
  full_loss <- loss(dat)#
  dat0 <- dat #
  dat0[, 1] <- dat[sample.int(n), 1]#
  null_loss <- loss(dat0)#
  cpi_hat <- mean(null_loss - full_loss)#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
       W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  null_3 <- c(null_distro, cpi_hat)
hist(null_3, breaks = 50)
library(data.table)
mat <- fread('~/Documents/QMUL/DietaryNitrates/otu_mini_mat.csv')
dim(mat)
mat[1:5, 1:5]
mat <- readRDS('~/Documents/QMUL/DietaryNitrates/OTUmat.rds')
mat[1:5, 1:5]
dim(mat)
mat <- fread('~/Documents/QMUL/DietaryNitrates/otu_mini_mat.csv')
mat <- as.matrix(mat[, taxlevel := NULL])
mat[1:5, 1:5]
clin <- fread('~/Documents/QMUL/DietaryNitrates/Clinical.csv')
# Drop outliers#
outliers <- c('Q04', 'Q12')#
mat <- mat[, !colnames(mat) %in% outliers]#
clin <- clin %>% filter(!Sample %in% outliers)#
#
# Filter out singleton OTUs#
keep <- rowSums(mat) > 1L#
mat <- mat[keep, ]
library(dplyr)
clin <- clin %>% filter(!Sample %in% outliers)
dim(mat)
mat <- read.csv('~/Documents/QMUL/DietaryNitrates/otu_mat_nitrate_reducers.csv')
mat[1:5, 1:5]
mat <- as.matrix(mat)
mat[1:5, 1:5]
saveRDS(mat, './Documents/QMUL/DietaryNitrates/otu_mat_nitrate_reducers.rds')
library(permute)#
library(data.table)#
library(doMC)#
registerDoMC(124)
library(permute)#
library(data.table)#
library(doMC)#
registerDoMC(4)
# I just want to check the null distributions tho...#
loop <- function(n, p, b) {#
  # Simulate data#
  x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
    W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  return(null_distro)#
}#
#
# Execute in parallel#
out <- foreach(b = seq_len(100), .combine = cbind) %dopar% loop(10, 3, b)
# I just want to check the null distributions tho...#
loop <- function(n, p, b) {#
  # Simulate data#
  x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }#
  # Estimate CPI#
  full_loss <- loss(dat)#
  dat0 <- dat #
  dat0[, 1] <- dat[sample.int(n), 1]#
  null_loss <- loss(dat0)#
  cpi_hat <- mean(null_loss - full_loss)#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
       W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  null_distro <- c(null_distro, cpi_hat)#
  return(null_distro)#
}#
#
# Execute in parallel#
out <- foreach(b = seq_len(100), .combine = cbind) %dopar% loop(10, 3, b)
dim(out)
hist(out[, 1], breaks = 100)
hist(out[, 2], breaks = 100)
hist(out[, 3], breaks = 100)
hist(out[, 4], breaks = 100)
hist(out[, 5], breaks = 100)
fn <- function(j) hist(out[, j], breaks = 100)
fn(6)
fn(7)
fn(8)
fn(9)
fn(10)
fn(11)
fn(12)
fn(13)
fn(14)
library(tidyverse)
df <- as.data.frame(out)
df <- tbl_df(out)
head(df)
ggplot(df, aes('result.1')) + geom_histogram()
ggplot(df, aes(result.1)) + geom_histogram()
fn <- function(j) {#
  colnames(df)[j] <- 'x'#
  ggplot(df, aes(x)) + #
    geom_histogram(bins = 100, color = 'black') + #
    geom_vline(xintercept = df$x[1024]) + #
    theme_bw()#
}
fn(1)
fn <- function(j) {#
  colnames(df)[j] <- 'x'#
  ggplot(df, aes(x)) + #
    geom_histogram(bins = 100, color = 'black') + #
    geom_vline(xintercept = df$x[1024], color = 'red') + #
    theme_bw()#
}
fn(1)
fn(2)
fn(3)
fn(4)
fn(5)
fn(6)
fn(7)
fn(8)
fn(9)
n <- 10
p <- 3
x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }
dat
fn(50)
fn(49)
df_1 <- df
loop <- function(n, p, b) {#
  # Simulate data#
  x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- runif(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }#
  # Estimate CPI#
  full_loss <- loss(dat)#
  dat0 <- dat #
  dat0[, 1] <- dat[sample.int(n), 1]#
  null_loss <- loss(dat0)#
  cpi_hat <- mean(null_loss - full_loss)#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
       W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  null_distro <- c(null_distro, cpi_hat)#
  return(null_distro)#
}#
#
# Execute in parallel#
out <- foreach(b = seq_len(100), .combine = cbind) %dopar% loop(10, 3, b)
df <- tbl_df(out)
fn(1)
fn
fn(2)
fn(3)
fn(4)
fn(5)
fn(6)
fn(7)
fn(8)
dat
f <- lm(y ~ ., data = dat)
names(f)
f$predictions
f$fitted.values
predictions(f)
predict(f)
fitted(f)
dim(df_1)
df_2 <- out
# I just want to check the null distributions tho...#
loop <- function(n, p, b) {#
  # Simulate data#
  x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Train models, calculate loss#
  f <- lm(y ~ ., data = dat)#
  full_loss <- (y - predict(f))^2#
  dat0 <- dat #
  dat0[, 1] <- dat[sample.int(n), 1]#
  f0 <- lm(y ~ ., data = dat0)#
  null_loss <- (y - predict(f0))^2#
  cpi_hat <- mean(null_loss - full_loss)#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
       W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  null_distro <- c(null_distro, cpi_hat)#
  return(null_distro)#
}#
#
# Execute in parallel#
out <- foreach(b = seq_len(100), .combine = cbind) %dopar% loop(10, 3, b)
df <- tbl_df(out)
fn(1)
fn(2)
fn(3)
fn(4)
fn(5)
fn(6)
fn(7)
fn(8)
mu <- colMeans(df)
hist(mu)
fn(1)
head(mu)
fn(2)
mean(mu)
mu <- colMeans(df_1)
mean(mu)
mean(colMeans(df_2))
fn
fn <- function(df, j) {#
  colnames(df)[j] <- 'x'#
  ggplot(df, aes(x)) + #
    geom_histogram(bins = 100, color = 'black') + #
    geom_vline(xintercept = df$x[1024], color = 'red') + #
    theme_bw()#
}
fn(df_1, 1)
fn(df_2, 1)
class(df_2)
df_2 <- tbl_df(df_2)
fn(df_2, 1)
fn(df_2, 2)
fn(df_2, 3)
fn(df_2, 5)
fn(df_2, 6)
fn(df_2, 7)
all
units <- rep(seq_len(10), 2)
w <- rep(c(1, 0), each = n)
n
ctrl <- how(within = Within(type = 'free', mirror = T), blocks = units, complete = T)
all <- allPerms(w, ctrl)
ctrl <- update(ctrl, all.perms = all)
all
names(all)
names(ctrl)
class(ctrl$all.perms)
ctrl$all.perms[1:5, 1:5]
dim(all)
fn(1)
fn(df_1, 1)
loop <- function(n, p, b) {#
  # Simulate data#
  x <- matrix(runif(n * p), ncol = p, #
              dimnames = list(NULL, paste0('x', seq_len(p))))#
  y <- rnorm(n)#
  dat <- data.frame(x, y)#
  # Cross-validated loss function#
  loss <- function(dat) {#
    err <- sapply(seq_len(n), function(i) {#
      train <- dat[-i, ]#
      test <- dat[i, ]#
      f <- lm(y ~ ., data = train)#
      (test$y - predict(f, test))^2#
    })#
    return(err)#
  }#
  # Estimate CPI#
  full_loss <- loss(dat)#
  null_loss <- loss(dat[, -1])#
  cpi_hat <- mean(null_loss - full_loss)#
  # Permute#
  df <- data.table(#
    Unit = rep(seq_len(n), times = 2),#
    W = rep(c(1, 0), each = n),#
    Loss = c(full_loss, null_loss)#
  )#
  ctrl <- how(within = Within(type = 'free', mirror = TRUE), #
              blocks = df$Unit, complete = TRUE)#
  all <- allPerms(df$W, control = ctrl)#
  ctrl <- update(ctrl, all.perms = all)#
  null_distro <- sapply(seq_len(2^n - 1), function(b) {#
    df[, W := W[permute(b, 2 * n, ctrl)]]#
    mean(df[W == 1, Loss] - df[W == 0, Loss])#
  })#
  null_distro <- c(null_distro, cpi_hat)#
  return(null_distro)#
}#
#
# Execute in parallel#
out <- foreach(b = seq_len(100), .combine = cbind) %dopar% loop(10, 3, b)
out <- data.frame(out)
fn(out, 1)
fn(out, 2)
fn(out, 3)
fn(out, 4)
fn(out, 5)
fn(out, 6)
fn(out, 7)
fn(out, 8)
fn(out, 9)
library(data.table)
library(ranger)
df <- fread('~/Documents/ThermoAI/Colbun/tg1.csv')
head(df)
colnames(df) <- gsub(' ', '.', colnames(df))
head(colnames(df))
lapply(df, class)
df[, Fecha := NULL]
dim(df)
?ranger
34/3
f <- ranger(Eficiencia ~ ., data = df, num.trees = 1000, mtry = 11)
f <- ranger(data = df, dependent.variable.name = 'Eficiencia', num.trees = 1000, mtry = 11)
f
library(doMC)
registerDoMC(4)
f <- ranger(data = df, dependent.variable.name = 'Eficiencia', num.trees = 1000, mtry = 11, importance = 'permutation', num.threads = 4)
importance(f)
saveRDS(f, './Documents/ThermoAI/Colbun/tg1_rf.rds')
?rnorm
1 * 1.13^5
# Set working directory#
setwd('./Documents/DPhil/Deaths_on_FB/Data/Final')#
#
# Load libraries#
library(data.table)#
library(tidyverse)#
library(mgcv)#
#
### Death Data ####
#
# Import UN data#
mort <- read_csv('mortality.csv') #
pop <- read_csv('population.csv') #
#
# For columns 3-23, delete the space and class as numeric#
for (j in 3:23) {#
  mort[[j]] <- gsub(' ', '', mort[[j]]) %>% as.numeric(.)#
  pop[[j]] <- gsub(' ', '', pop[[j]]) %>% as.numeric(.)#
}#
#
# Make sure both data frames have all and only the same columns #
mort <- mort %>% select(-Total)#
pop <- pop %>% #
  mutate(Age_95 = Age_95 + Age_100) %>%#
  select(-Age_100)#
#
# Fix Time column in mort#
mort <- mort %>% #
  mutate(Time = gsub(' -.*', '', Time) %>% as.integer(.))#
#
# Tidy data, reclass age as integer#
mort <- mort %>% #
  gather(Age, Deaths, -Time, -Location) %>%#
  mutate(Age = gsub('Age_', '', Age) %>% as.integer(.),#
      Deaths = Deaths / 5,#
         Idx = paste(Location, Time, Age, sep = '.')) %>%#
  as.data.table(.)#
pop <- pop %>% #
  gather(Age, Population, -Time, -Location) %>%#
  mutate(Age = gsub('Age_', '', Age) %>% as.integer(.),#
         Idx = paste(Location, Time, Age, sep = '.')) %>%#
  as.data.table(.)#
#
# Merge data, calculate mortality rate, Winsorize distribution#
un_dat <- merge(mort, pop[, .(Idx, Population)], by = 'Idx'#
  )[, Idx := NULL#
# Calculate mortality rate#
  ][, Mortality_Rate := Deaths / Population#
# Winsorize the distribution#
  ][Mortality_Rate >= 0.99, Mortality_Rate := 0.99#
# Logit transform for easier modelling#
  ][, logit_mr := qlogis(Mortality_Rate + 1e-6)#
# Fix a few country names #
  ][Location == 'Viet Nam', Location := 'Vietnam'#
  ][Location == 'United States of America', Location := 'United States']#
#
# Export#
saveRDS(un_dat, 'un_dat.rds')#
#
### Facebook Data ####
#
# Import Facebook data#
fb <- read_csv('fb_dat.csv') %>%#
# Remove the 65+ bucket#
  filter(Age != 65) %>%#
  as.data.table(.)#
# Anchor Facebook numbers with a guaranteed 0 users at age 100#
anchor <- data.table(#
  Country = fb[, unique(Country)],#
      Age = 100,#
    Users = 0#
)#
fb <- rbind(fb, anchor)#
#
# Export#
saveRDS(fb, 'fb_dat.rds')
fb_dat <- fb
fb_dat
un_dat
# Build death model#
death_mod <- gam(logit_mr ~ s(Age, by = Time), #
                 data = un_dat[Age >= 10 & Location == 'United States'])#
#
# Build Facebook model#
fb_mod <- gam(Users ~ s(Age), #
              data = fb_dat[Country == 'United States'])#
#
# Fill in 2018 data, trim distributions#
df <- data.table(#
  Time = 2018, #
   Age = 13:100#
)#
df[, mr_hat := predict(death_mod, df) %>% plogis(.)#
  ][, fb_hat := predict(fb_mod, df)#
  ][mr_hat > 1, mr_hat := 1#
  ][mr_hat < 0, mr_hat := 0#
  ][fb_hat < 0, fb_hat := 0]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- df[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, fb_hat := last_df[, fb_hat * (1 - mr_hat)]]#
  df <- rbind(df, new_df)#
}#
#
# Plot "real" vs. projected mortality rates#
dat <- merge(df[Time == 2050], #
             un_dat[Location == 'United States' & Time == 2050 & Age >= 10, #
                    .(Age, Mortality_Rate)],#
             by = 'Age')
dat
df
dat <- merge(df[Time == 2030], #
             un_dat[Location == 'United States' & Time == 2030 & Age >= 10, #
                    .(Age, Mortality_Rate)],#
             by = 'Age')
dat
df
dat
dat %>% gather(Age, Value, -Time, -fb_hat) %>% head(.)
dat %>% gather(Value, Age, -Time, -fb_hat) %>% head(.)
dat %>% gather(Mortality_Rate, Age, -Time, -fb_hat) %>% head(.)
dat <- merge(df[Time == 2030, .(Time, Age, mr_hat)], #
             un_dat[Location == 'United States' & Time == 2030 & Age >= 10, #
                    .(Age, Mortality_Rate)],#
             by = 'Age')
dat
?gather
iris
mini_iris <- iris[c(1, 51, 101), ]#
# gather Sepal.Length, Sepal.Width, Petal.Length, Petal.Width#
gather(mini_iris, key = flower_att, value = measurement,#
       Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
dat
dat %>% gather(key = type, value = mr, mr_hat, Mortality_rate)
dat %>% gather(key = type, value = mr, mr_hat, Mortality_Rate)
dat2 <- data.table(#
  Time = rep(dat$Time, 2),#
   Age = rep(dat$Age, 2),#
    MR = c(dat$Mortality_Rate, dat$mr_hat),#
  Type = rep(c('Observed', 'Predicted'), each = 17)#
)
dat2
?rnorm
ggplot(dat) + #
  geom_point(dat %>% filter(Type == 'Observed'), aes(Age, MR)) + #
  geom_line(dat %>% filter(Type == 'Predicted'), aes(Age, MR), #
            size = 0.75, color = 'blue') + #
  theme_bw()
dat
dat <- data.table(#
  Time = rep(dat$Time, 2),#
   Age = rep(dat$Age, 2),#
    MR = c(dat$Mortality_Rate, dat$mr_hat),#
  Type = rep(c('Observed', 'Predicted'), each = 17)#
)
dat
ggplot(dat) + #
  geom_point(data = filter(dat, Type == 'Observed'), aes(Age, MR)) + #
  geom_line(data = filter(dat, Type == 'Predicted'), aes(Age, MR), #
            size = 0.75, color = 'blue') + #
  theme_bw()
# Fill in 2018 data, trim distributions#
df <- data.table(#
  Time = 2018, #
   Age = 13:100#
)#
df[, mr_hat := predict(death_mod, df) %>% plogis(.)#
  ][, fb_hat := predict(fb_mod, df)#
  ][mr_hat > 1, mr_hat := 1#
  ][mr_hat < 0, mr_hat := 0#
  ][fb_hat < 0, fb_hat := 0]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- df[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, fb_hat := last_df[, fb_hat * (1 - mr_hat)]]#
  df <- rbind(df, new_df)#
}
df
un_dat
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate)) + #
  geom_line(data = df %>% filter(Time == 2030), aes(Age, mr_hat),#
            size = 0.75, color = 'blue') + #
  theme_bw()
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate)) + #
  geom_line(data = df %>% filter(Time == 2030), aes(Age, mr_hat),#
            size = 0.75, color = 'blue') + #
  theme_bw()
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate * 1000)) + #
  geom_line(data = df %>% filter(Time == 2030), aes(Age, mr_hat),#
            size = 0.75, color = 'blue') + #
  #labs(title = '')#
  theme_bw()
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate * 1000)) + #
  geom_line(data = df %>% filter(Time == 2030), #
            aes(Age, mr_hat * 1000),#
            size = 0.75, color = 'blue') + #
  #labs(title = '')#
  theme_bw()
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate * 1000)) + #
  geom_line(data = df %>% filter(Time == 2030), #
            aes(Age, mr_hat * 1000),#
            size = 0.75, color = 'blue') + #
  labs(title = 'Projected Mortality: United States, 2030',#
           y = 'Deaths per Thousand') +#
  theme_bw() + #
  theme(plot.title = element_text(hjust = 0.5))
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate * 1000)) + #
  geom_line(data = df %>% filter(Time == 2030), #
            aes(Age, mr_hat * 1000),#
            size = 0.75, color = 'blue') + #
  labs(title = 'Projected Mortality: United States, 2030',#
           y = 'Deaths per Thousand') +#
  theme_bw() + #
  theme(plot.title = element_text(hjust = 0.5))
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate * 1000)) + #
  geom_line(data = df %>% filter(Time == 2030), #
            aes(Age, mr_hat * 1000),#
            size = 0.75, color = 'blue') + #
  labs(title = 'Projected Mortality: United States, 2030',#
           y = 'Deaths Per Thousand') +#
  theme_bw() + #
  theme(plot.title = element_text(hjust = 0.5))
ggplot() + #
  geom_point(data = un_dat %>% filter(Location == 'United States',#
                                          Time == 2030,#
                                           Age >= 10), #
             aes(Age, Mortality_Rate * 1000)) + #
  geom_line(data = df %>% filter(Time == 2030,#
                                  Age <= 95), #
            aes(Age, mr_hat * 1000),#
            size = 0.75, color = 'blue') + #
  labs(title = 'Projected Mortality: United States, 2030',#
           y = 'Deaths Per Thousand') +#
  theme_bw() + #
  theme(plot.title = element_text(hjust = 0.5))
fb_dat
df
dat <- fb_dat[Country == 'United States']#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- dat[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, Users := last_df[, Users * (1 - mr_hat)]]#
  dat <- rbind(dat, new_df)#
}
dat
dat <- fb_dat[Country == 'United States'#
  ][, Time = 2018]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- dat[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, Users := last_df[, Users * (1 - mr_hat)]]#
  dat <- rbind(dat, new_df)#
}
dat <- fb_dat %>%#
  filter(Country == 'United States') %>%#
  mutate(Time = 2018)
dat <- fb_dat %>%#
  filter(Country == 'United States') %>%#
  mutate(Time = 2018) %>%#
  as.data.table(.)#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- dat[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, Users := last_df[, Users * (1 - mr_hat)]]#
  dat <- rbind(dat, new_df)#
}
dat
### Figure 2 ####
dat <- fb_dat %>%#
  filter(Country == 'United States') %>%#
  mutate(Time = 2018) %>%#
  select(Time, Age, Users) %>%#
  as.data.table(.)#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- dat[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, Users := last_df[, Users * (1 - mr_hat)]#
    ][, .(Time, Age, Users)]#
  dat <- rbind(dat, new_df)#
}
dat
### Figure 2 ####
dat <- fb_dat %>%#
  filter(Country == 'United States') %>%#
  mutate(Time = 2018) %>%#
  select(Time, Age, Users) %>%#
  as.data.table(.)#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- dat[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, Users := last_df[, Users * (1 - mr_hat)]]#
  new_df <- new_df[, .(Time, Age, Users)]#
  dat <- rbind(dat, new_df)#
}
dat
dat <- fb_dat %>%#
  filter(Country == 'United States') %>%#
  mutate(Time = 2018) %>%#
  select(Time, Age, Users) %>%#
  as.data.table(.)
last_df <- dat
dat
new_df <- data.table(#
    Time = 2019,#
     Age = 13:100#
  )
# Fill in 2018 data, trim distributions#
df <- data.table(#
  Time = 2018, #
   Age = 13:100#
)#
df[, mr_hat := predict(death_mod, df) %>% plogis(.)#
  ][, fb_hat := predict(fb_mod, df)#
  ][mr_hat > 1, mr_hat := 1#
  ][mr_hat < 0, mr_hat := 0#
  ][fb_hat < 0, fb_hat := 0]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- df[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, fb_hat := last_df[, fb_hat * (1 - mr_hat)]]#
  df <- rbind(df, new_df)#
}
df
dat <- df %>% filter(Time == 2018) %>% inner_join(fb_dat %>% filter(Country == 'United States'), by = 'Age')
dat
# Fill in 2018 data, trim distributions#
df <- data.table(#
  Time = 2018, #
   Age = 13:100#
)#
df[, mr_hat := predict(death_mod, df) %>% plogis(.)#
  ][, fb_hat := predict(fb_mod, df)#
  ][mr_hat > 1, mr_hat := 1#
  ][mr_hat < 0, mr_hat := 0#
  ][fb_hat < 0, fb_hat := 0]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- df[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, fb_hat := last_df[, fb_hat * (1 - mr_hat)]]#
  df <- rbind(df, new_df)#
}
df
df[Time == 2018, Users := fb_dat[Country == 'United States', Users]]
# Fill in 2018 data, trim distributions#
df <- data.table(#
  Time = 2018, #
   Age = 13:100#
)#
df[, mr_hat := predict(death_mod, df) %>% plogis(.)#
  ][, fb_hat := predict(fb_mod, df)#
  ][mr_hat > 1, mr_hat := 1#
  ][mr_hat < 0, mr_hat := 0#
  ][fb_hat < 0, fb_hat := 0]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- df[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, fb_hat := last_df[, fb_hat * (1 - mr_hat)]]#
  df <- rbind(df, new_df)#
}
dim(df)
dim(df[Time == 2018, ])
fb_dat[Country == 'United States']
df
df[Time == 2018 & Age %in% c(13:64, 100), #
   Users := fb_dat[Country == 'United States', Users]]
# Fill in 2018 data, trim distributions#
df <- data.table(#
  Time = 2018, #
   Age = 13:100#
)#
df[, mr_hat := predict(death_mod, df) %>% plogis(.)#
  ][, fb_hat := predict(fb_mod, df)#
  ][mr_hat > 1, mr_hat := 1#
  ][mr_hat < 0, mr_hat := 0#
  ][fb_hat < 0, fb_hat := 0]#
#
# Assume shrinking penetration rates#
for (year in 2019:2030) {#
  last_df <- df[Time == year - 1]#
  new_df <- data.table(#
    Time = year,#
     Age = 13:100#
  )#
  new_df[, mr_hat := predict(death_mod, new_df) %>% plogis(.)#
    ][mr_hat > 1, mr_hat := 1#
    ][mr_hat < 0, mr_hat := 0#
    ][, fb_hat := last_df[, fb_hat * (1 - mr_hat)]]#
  df <- rbind(df, new_df)#
}
df[Time == 2018 & Age %in% c(18:64, 100), #
   Users := fb_dat[Country == 'United States', Users]]
df
head(df, 20)
df[Age == 100, ]
